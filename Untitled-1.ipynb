{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fb145e0",
   "metadata": {},
   "source": [
    "# 1. Загрузка и подготовка данных\n",
    "\n",
    "В этой части мы:\n",
    "- загружаем датасет;\n",
    "- приводим имена столбцов к аккуратному виду;\n",
    "- смотрим базовую информацию (размерность, типы столбцов, наличие пропусков);\n",
    "- визуализируем распределение целевой переменной `Method`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478231c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Загрузка и подготовка данных\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, KBinsDiscretizer, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "import random\n",
    "\n",
    "# Для воспроизводимости\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Имя файла датасета (лежит в той же папке, что и ноутбук)\n",
    "DATA_PATH = \"International Study on Male Genital Measurements Dataset.csv\"\n",
    "\n",
    "# Загрузка (важно: здесь у датасета не-UTF8 кодировка)\n",
    "df = pd.read_csv(DATA_PATH, encoding=\"latin1\")\n",
    "\n",
    "# Убираем лишние пробелы в именах столбцов\n",
    "df.columns = [col.strip() for col in df.columns]\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452322d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x=\"Method\", data=df)\n",
    "plt.title(\"Распределение целевой переменной Method\")\n",
    "plt.xlabel(\"Method\")\n",
    "plt.ylabel(\"Количество стран\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc2a6d",
   "metadata": {},
   "source": [
    "## 2. Формирование признаков и разбиение выборки\n",
    "\n",
    "В качестве целевой переменной (`y`) используем столбец `Method`, \n",
    "который кодируем в 0/1 с помощью `LabelEncoder`.\n",
    "\n",
    "В качестве признаков (`X`) используем:\n",
    "- числовые антропометрические признаки;\n",
    "- размер выборки `N`;\n",
    "- регион `Region` (категориальный признак).\n",
    "\n",
    "Столбец `Country` не используем как признак, так как это просто название страны.\n",
    "\n",
    "Далее разбиваем выборку на обучающую и тестовую, используя `train_test_split`\n",
    "с параметрами `test_size=0.25` и `stratify=y`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ddd313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список числовых признаков\n",
    "numeric_features = [\n",
    "    \"Flaccid Length (cm)\",\n",
    "    \"Erect Length (cm)\",\n",
    "    \"Flaccid Circumference (cm)\",\n",
    "    \"Erect Circumference (cm)\",\n",
    "    \"Flaccid Volume (cm³)\",\n",
    "    \"Erect Volume (cm³)\",\n",
    "    \"Growth Length\",\n",
    "    \"Growth Circumference\",\n",
    "    \"Growth Volume\",\n",
    "    \"N\"\n",
    "]\n",
    "\n",
    "# Категориальные признаки\n",
    "categorical_features = [\"Region\"]\n",
    "\n",
    "# Формируем матрицу признаков X и целевой вектор y\n",
    "X = df[numeric_features + categorical_features]\n",
    "y_raw = df[\"Method\"]\n",
    "\n",
    "# Кодируем целевую переменную в 0/1\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y_raw)\n",
    "\n",
    "print(\"Соответствие классов:\", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
    "\n",
    "# Разбиение на train и test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.25,\n",
    "    random_state=SEED,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Размер X_train:\", X_train.shape)\n",
    "print(\"Размер X_test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5586e18",
   "metadata": {},
   "source": [
    "## 3. Препроцессинг признаков\n",
    "\n",
    "Для разных классификаторов удобны разные масштабы признаков:\n",
    "\n",
    "- Для большинства моделей (SVM, kNN, LDA, Decision Tree, GaussianNB) \n",
    "  используем стандартизацию числовых признаков (`StandardScaler`) \n",
    "  и one-hot кодирование региона.\n",
    "\n",
    "- Для `MultinomialNB` и `ComplementNB` важны неотрицательные признаки, \n",
    "  поэтому используем `MinMaxScaler` (0–1) + one-hot кодирование.\n",
    "\n",
    "- Для `BernoulliNB` удобны бинарные признаки, \n",
    "  поэтому используем `KBinsDiscretizer` с двумя бинами (0/1) \n",
    "  и one-hot кодирование региона.\n",
    "\n",
    "Все эти преобразования реализуем через `ColumnTransformer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1c9fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, KBinsDiscretizer\n",
    "\n",
    "# Препроцессинг для \"обычных\" моделей (SVM, kNN, LDA, DecisionTree, GaussianNB)\n",
    "preprocessor_std = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Препроцессинг для MultinomialNB и ComplementNB (неотрицательные значения)\n",
    "preprocessor_minmax = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", MinMaxScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Препроцессинг для BernoulliNB (бинарные признаки)\n",
    "preprocessor_bin = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", KBinsDiscretizer(n_bins=2, encode=\"onehot-dense\", strategy=\"quantile\"), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4194f8a1",
   "metadata": {},
   "source": [
    "## 4. Базовые модели классификации\n",
    "\n",
    "Здесь строим и оцениваем модели:\n",
    "\n",
    "1. Наивный байесовский классификатор:\n",
    "   - `GaussianNB`\n",
    "   - `MultinomialNB`\n",
    "   - `ComplementNB`\n",
    "   - `BernoulliNB`\n",
    "2. Дерево решений (`DecisionTreeClassifier`)\n",
    "3. Линейный дискриминантный анализ (`LinearDiscriminantAnalysis`)\n",
    "4. Метод опорных векторов (`SVC`)\n",
    "5. Метод k-ближайших соседей (`KNeighborsClassifier`)\n",
    "\n",
    "Для каждой модели вычисляем:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-score\n",
    "- ROC-AUC\n",
    "\n",
    "Также будем сохранять скоры для построения ROC-кривых.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eb6719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(name, model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Обучает модель, вычисляет метрики и возвращает их вместе с ROC-данными.\n",
    "    \"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Предсказанные \"скор\" для ROC (вероятности или decision_function)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_scores = model.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        y_scores = model.decision_function(X_test)\n",
    "    else:\n",
    "        y_scores = None\n",
    "\n",
    "    metrics = {\n",
    "        \"model\": name,\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred),\n",
    "        \"recall\": recall_score(y_test, y_pred),\n",
    "        \"f1\": f1_score(y_test, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_scores) if y_scores is not None else np.nan,\n",
    "    }\n",
    "\n",
    "    return metrics, y_pred, y_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94de96b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Наивные Байесовские классификаторы\n",
    "pipe_gnb = Pipeline([\n",
    "    (\"preprocess\", preprocessor_std),\n",
    "    (\"model\", GaussianNB())\n",
    "])\n",
    "\n",
    "pipe_mnb = Pipeline([\n",
    "    (\"preprocess\", preprocessor_minmax),\n",
    "    (\"model\", MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_cnb = Pipeline([\n",
    "    (\"preprocess\", preprocessor_minmax),\n",
    "    (\"model\", ComplementNB())\n",
    "])\n",
    "\n",
    "pipe_bnb = Pipeline([\n",
    "    (\"preprocess\", preprocessor_bin),\n",
    "    (\"model\", BernoulliNB())\n",
    "])\n",
    "\n",
    "# Остальные модели\n",
    "pipe_dt = Pipeline([\n",
    "    (\"preprocess\", preprocessor_std),\n",
    "    (\"model\", DecisionTreeClassifier(random_state=SEED))\n",
    "])\n",
    "\n",
    "pipe_lda = Pipeline([\n",
    "    (\"preprocess\", preprocessor_std),\n",
    "    (\"model\", LinearDiscriminantAnalysis())\n",
    "])\n",
    "\n",
    "pipe_svc = Pipeline([\n",
    "    (\"preprocess\", preprocessor_std),\n",
    "    (\"model\", SVC(kernel=\"rbf\", probability=True, random_state=SEED))\n",
    "])\n",
    "\n",
    "pipe_knn = Pipeline([\n",
    "    (\"preprocess\", preprocessor_std),\n",
    "    (\"model\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "base_models = {\n",
    "    \"GaussianNB\": pipe_gnb,\n",
    "    \"MultinomialNB\": pipe_mnb,\n",
    "    \"ComplementNB\": pipe_cnb,\n",
    "    \"BernoulliNB\": pipe_bnb,\n",
    "    \"DecisionTree\": pipe_dt,\n",
    "    \"LDA\": pipe_lda,\n",
    "    \"SVM (RBF)\": pipe_svc,\n",
    "    \"kNN\": pipe_knn,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4078b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_base = []\n",
    "roc_data_base = {}\n",
    "\n",
    "for name, model in base_models.items():\n",
    "    metrics, y_pred, y_scores = evaluate_classifier(\n",
    "        name, model, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "    results_base.append(metrics)\n",
    "    if y_scores is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "        roc_data_base[name] = (fpr, tpr, metrics[\"roc_auc\"])\n",
    "\n",
    "results_base_df = pd.DataFrame(results_base).sort_values(by=\"f1\", ascending=False)\n",
    "results_base_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d9c1f9",
   "metadata": {},
   "source": [
    "## 5. Визуализация качества классификаторов\n",
    "\n",
    "Здесь мы:\n",
    "\n",
    "1. Строим столбчатую диаграмму по метрикам \n",
    "   (Accuracy, Precision, Recall, F1, ROC-AUC) для всех базовых моделей.\n",
    "2. Строим ROC-кривые для лучшего поднабора моделей.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a04a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_plot = [\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "results_base_df.set_index(\"model\")[metrics_to_plot].plot(\n",
    "    kind=\"bar\",\n",
    "    figsize=(12, 6)\n",
    ")\n",
    "plt.title(\"Сравнение метрик базовых моделей\")\n",
    "plt.ylabel(\"Значение метрики\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3134adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for name, (fpr, tpr, auc_val) in roc_data_base.items():\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {auc_val:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Случайный классификатор\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC-кривые базовых моделей\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461f7ebe",
   "metadata": {},
   "source": [
    "## 6. Настройка гиперпараметров\n",
    "\n",
    "В этом разделе мы подбираем гиперпараметры для основных моделей:\n",
    "\n",
    "- `GaussianNB`: параметр `var_smoothing`\n",
    "- `DecisionTreeClassifier`: глубина дерева, минимальный размер сплита\n",
    "- `LinearDiscriminantAnalysis`: solver\n",
    "- `SVC`: параметр регуляризации `C` и параметр ядра `gamma`\n",
    "- `KNeighborsClassifier`: число соседей, схема взвешивания\n",
    "\n",
    "Подбор выполняется с помощью `GridSearchCV` с кросс-валидацией (cv=5) \n",
    "и метрикой качества `f1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d7a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_results = []\n",
    "roc_data_tuned = {}\n",
    "best_models = {}\n",
    "\n",
    "# 6.1. GaussianNB\n",
    "pipe_gnb_gs = Pipeline([\n",
    "    (\"preprocess\", preprocessor_std),\n",
    "    (\"model\", GaussianNB())\n",
    "])\n",
    "\n",
    "param_grid_gnb = {\n",
    "    \"model__var_smoothing\": [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "}\n",
    "\n",
    "gs_gnb = GridSearchCV(\n",
    "    pipe_gnb_gs,\n",
    "    param_grid_gnb,\n",
    "    cv=5,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "gs_gnb.fit(X_train, y_train)\n",
    "best_models[\"GaussianNB_tuned\"] = gs_gnb.best_estimator_\n",
    "\n",
    "# 6.2. DecisionTree\n",
    "pipe_dt_gs = Pipeline([\n",
    "    (\"preprocess\", preprocessor_std),\n",
    "    (\"model\", DecisionTreeClassifier(random_state=SEED))\n",
    "])\n",
    "\n",
    "param_grid_dt = {\n",
    "    \"model__max_depth\": [3, 4, 5, None],\n",
    "    \"model__min_samples_split\": [2, 4, 6]\n",
    "}\n",
    "\n",
    "gs_dt = GridSearchCV(\n",
    "    pipe_dt_gs,\n",
    "    param_grid_dt,\n",
    "    cv=5,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "gs_dt.fit(X_train, y_train)\n",
    "best_models[\"DecisionTree_tuned\"] = gs_dt.best_estimator_\n",
    "\n",
    "# 6.3. LDA\n",
    "pipe_lda_gs = Pipeline([\n",
    "    (\"preprocess\", preprocessor_std),\n",
    "    (\"model\", LinearDiscriminantAnalysis())\n",
    "])\n",
    "\n",
    "param_grid_lda = {\n",
    "    \"model__solver\": [\"svd\", \"lsqr\", \"eigen\"]\n",
    "}\n",
    "\n",
    "gs_lda = GridSearchCV(\n",
    "    pipe_lda_gs,\n",
    "    param_grid_lda,\n",
    "    cv=5,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "gs_lda.fit(X_train, y_train)\n",
    "best_models[\"LDA_tuned\"] = gs_lda.best_estimator_\n",
    "\n",
    "# 6.4. SVM\n",
    "pipe_svc_gs = Pipeline([\n",
    "    (\"preprocess\", preprocessor_std),\n",
    "    (\"model\", SVC(probability=True, random_state=SEED))\n",
    "])\n",
    "\n",
    "param_grid_svc = {\n",
    "    \"model__kernel\": [\"linear\", \"rbf\"],\n",
    "    \"model__C\": [0.1, 1.0, 10.0],\n",
    "    \"model__gamma\": [\"scale\", \"auto\"]\n",
    "}\n",
    "\n",
    "gs_svc = GridSearchCV(\n",
    "    pipe_svc_gs,\n",
    "    param_grid_svc,\n",
    "    cv=5,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "gs_svc.fit(X_train, y_train)\n",
    "best_models[\"SVM_tuned\"] = gs_svc.best_estimator_\n",
    "\n",
    "# 6.5. kNN\n",
    "pipe_knn_gs = Pipeline([\n",
    "    (\"preprocess\", preprocessor_std),\n",
    "    (\"model\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "param_grid_knn = {\n",
    "    \"model__n_neighbors\": [3, 5, 7, 9],\n",
    "    \"model__weights\": [\"uniform\", \"distance\"]\n",
    "}\n",
    "\n",
    "gs_knn = GridSearchCV(\n",
    "    pipe_knn_gs,\n",
    "    param_grid_knn,\n",
    "    cv=5,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "gs_knn.fit(X_train, y_train)\n",
    "best_models[\"kNN_tuned\"] = gs_knn.best_estimator_\n",
    "\n",
    "# Оценка настроенных моделей\n",
    "for name, model in best_models.items():\n",
    "    metrics, y_pred, y_scores = evaluate_classifier(\n",
    "        name, model, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "    tuned_results.append(metrics)\n",
    "    if y_scores is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "        roc_data_tuned[name] = (fpr, tpr, metrics[\"roc_auc\"])\n",
    "\n",
    "tuned_results_df = pd.DataFrame(tuned_results).sort_values(by=\"f1\", ascending=False)\n",
    "tuned_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20bc8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Базовые модели:\")\n",
    "display(results_base_df)\n",
    "\n",
    "print(\"\\nМодели с подобранными гиперпараметрами:\")\n",
    "display(tuned_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdec105",
   "metadata": {},
   "source": [
    "## 7. ROC-кривые для настроенных моделей\n",
    "\n",
    "Строим ROC-кривые для моделей с подобранными гиперпараметрами \n",
    "и сравниваем их между собой.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8870f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for name, (fpr, tpr, auc_val) in roc_data_tuned.items():\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {auc_val:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Случайный классификатор\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC-кривые моделей с подобранными гиперпараметрами\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1537b86",
   "metadata": {},
   "source": [
    "## 8. Нейронная сеть на TensorFlow и визуализация в TensorBoard\n",
    "\n",
    "В этой части мы:\n",
    "\n",
    "1. Повторно готовим признаки для нейронной сети:\n",
    "   - стандартизация числовых признаков;\n",
    "   - one-hot кодирование категориальных признаков.\n",
    "\n",
    "2. Строим полносвязную нейронную сеть (MLP) в Keras:\n",
    "   - входной слой;\n",
    "   - несколько скрытых слоёв с ReLU;\n",
    "   - выходной слой с сигмоидой (бинарная классификация).\n",
    "\n",
    "3. Компилируем модель с функцией ошибки `binary_crossentropy`\n",
    "   и метриками `accuracy` и `AUC`.\n",
    "\n",
    "4. Подключаем коллбек `TensorBoard` для логирования:\n",
    "   - `loss`, `accuracy`, `val_loss`, `val_accuracy`.\n",
    "\n",
    "5. Обучаем модель и визуализируем кривые обучения.\n",
    "\n",
    "6. Оцениваем качество на тестовой выборке и сравниваем с классическими моделями.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa97fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Препроцессинг для нейросети: стандартизация + one-hot\n",
    "nn_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Обучаем препроцессор на train и применяем к train/test\n",
    "X_train_nn = nn_preprocessor.fit_transform(X_train)\n",
    "X_test_nn = nn_preprocessor.transform(X_test)\n",
    "\n",
    "y_train_nn = y_train.astype(\"float32\")\n",
    "y_test_nn = y_test.astype(\"float32\")\n",
    "\n",
    "X_train_nn.shape, X_test_nn.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0089a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "input_dim = X_train_nn.shape[1]\n",
    "\n",
    "def build_model(hidden_units=32, hidden_units2=16, dropout_rate=0.2, learning_rate=1e-3):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(hidden_units, activation=\"relu\"),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(hidden_units2, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\n",
    "            \"accuracy\",\n",
    "            tf.keras.metrics.AUC(name=\"auc\")\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "log_dir = \"logs/method_classifier\"\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1\n",
    ")\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_nn,\n",
    "    y_train_nn,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    callbacks=[tensorboard_cb, early_stopping_cb],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ade712e",
   "metadata": {},
   "source": [
    "Для запуска TensorBoard из консоли VS Code:\n",
    "\n",
    "tensorboard --logdir logs/method_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9423c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_df[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history_df[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Кривые loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_df[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(history_df[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Кривые accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85329e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_auc = model.evaluate(X_test_nn, y_test_nn, verbose=0)\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "# Предсказания и классический набор метрик\n",
    "y_proba_nn = model.predict(X_test_nn).ravel()\n",
    "y_pred_nn = (y_proba_nn >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification report (нейросеть):\")\n",
    "print(classification_report(y_test, y_pred_nn, target_names=label_encoder.classes_))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, y_pred_nn,\n",
    "    display_labels=label_encoder.classes_,\n",
    "    cmap=\"Blues\"\n",
    ")\n",
    "plt.title(\"Confusion matrix для нейросети\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC-кривая для нейросети\n",
    "fpr_nn, tpr_nn, _ = roc_curve(y_test, y_proba_nn)\n",
    "auc_nn = roc_auc_score(y_test, y_proba_nn)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr_nn, tpr_nn, label=f\"Нейросеть (AUC = {auc_nn:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Случайный классификатор\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC-кривая нейросети\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b26f7",
   "metadata": {},
   "source": [
    "## 9. Влияние гиперпараметров нейросети\n",
    "\n",
    "Для исследования влияния гиперпараметров меняем:\n",
    "- число нейронов в скрытых слоях,\n",
    "- dropout,\n",
    "- скорость обучения.\n",
    "\n",
    "Сравниваем качество по метрикам Accuracy и AUC на тестовой выборке.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb5c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    {\"hidden_units\": 16, \"hidden_units2\": 8, \"dropout_rate\": 0.1, \"learning_rate\": 1e-3},\n",
    "    {\"hidden_units\": 32, \"hidden_units2\": 16, \"dropout_rate\": 0.2, \"learning_rate\": 1e-3},\n",
    "    {\"hidden_units\": 64, \"hidden_units2\": 32, \"dropout_rate\": 0.3, \"learning_rate\": 5e-4},\n",
    "]\n",
    "\n",
    "nn_results = []\n",
    "\n",
    "for cfg in configs:\n",
    "    print(\"\\nКонфигурация:\", cfg)\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "    model_cfg = build_model(\n",
    "        hidden_units=cfg[\"hidden_units\"],\n",
    "        hidden_units2=cfg[\"hidden_units2\"],\n",
    "        dropout_rate=cfg[\"dropout_rate\"],\n",
    "        learning_rate=cfg[\"learning_rate\"]\n",
    "    )\n",
    "\n",
    "    history_cfg = model_cfg.fit(\n",
    "        X_train_nn,\n",
    "        y_train_nn,\n",
    "        validation_split=0.2,\n",
    "        epochs=60,\n",
    "        batch_size=16,\n",
    "        callbacks=[early_stopping_cb],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    loss, acc, auc_val = model_cfg.evaluate(X_test_nn, y_test_nn, verbose=0)\n",
    "    print(f\"Test Accuracy: {acc:.4f}, Test AUC: {auc_val:.4f}\")\n",
    "\n",
    "    nn_results.append({\n",
    "        \"hidden_units\": cfg[\"hidden_units\"],\n",
    "        \"hidden_units2\": cfg[\"hidden_units2\"],\n",
    "        \"dropout_rate\": cfg[\"dropout_rate\"],\n",
    "        \"learning_rate\": cfg[\"learning_rate\"],\n",
    "        \"test_accuracy\": acc,\n",
    "        \"test_auc\": auc_val\n",
    "    })\n",
    "\n",
    "nn_results_df = pd.DataFrame(nn_results)\n",
    "nn_results_df\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
